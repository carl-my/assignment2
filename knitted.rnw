\documentclass{article}
\title{Statistical Programming with R\\Assignment 2}

\author{\small Tove Henning\\ \small Carl Munkby\\ \small Johannes Zetterberg}
\date{}

\usepackage[margin = 1in]{geometry}


\begin{document}
\maketitle

\pagebreak

\section{Least squares variance simulation}
<<>>=
library(StatProg)
library(tidyverse)

#### OLS
olsFun <- function(data){
  ### create variable for the number of observations in the dataset
  N <- nrow(data)
  
  ### set column names and add intercept column to X
  Y <- data[,1]
  X <- cbind(rep(1, N), data[,2])
  
  ### calculate the formel and extract the Beta coefficient 
  beta_ols=(solve(t(X)%*% X) %*% (t(X) %*% Y))[2,1]
  
  return(beta_ols)
}
@

To estimate the beta for the OLS regression we set column one of the dataset to Y and column two to X. Adding an extra intercept column of N rows to X. Lastly we calculate beta by using the formel and extract the value for beta coefficient.

<<>>=
##### weighted least squares
wlsFun <- function(data, lambda){
  if (is.numeric(lambda)==FALSE){print("lambda is not numeric") } 
  else{
    ### create variable for the number of observations in the dataset
    N <- nrow(data)
    
    ### set column names and add intercept column for X
    Y <- data[,1]
    X <- cbind(rep(1,N), data[,2])
    
    ### create a zero matrix N x N
    Z <- matrix(0, N, N)
    
    ## make a forloop to put in the error terms on the diagonal 
    ## to create the error covariance matrix
    er <- NULL
    for (i in 1:N) {
      er[i] <- exp(X[i,2]*lambda)
      Z[i,i] <- er[i] 
    }
    ### calculate and extract the Beta coefficient
    beta_wls = ((solve(t(X)%*%(solve(Z))%*%X)%*%t(X)%*%
                   (solve(Z))%*%Y))[2,1]
  return(beta_wls)
  }
}
@

To estimate beta for the weighted least squares estimator column one of the dataset has been set to Y 
and column two to X. Then an extra intercept column of N rows has been added to X. To be able to get
the error covariance matrix we added a zero matrix of size N x N. Then using the zero matrix in a forloop
and taking the exponent of x times lambda on the diagonal. 

\subsubsection*{Feasible weighted least squares}

<<>>=

fwlsFun <- function(data, trueVar){
  y = data[,1]                             # y is given the value of the first column in the data  
  N <- nrow(data)                          # N is given the value of the number of rows in the data
  X = cbind(rep(1,N), data[,2:ncol(data)]) # X is given the value of the remaining columns in the data 
                                           # and an intercept is added
  
  mod = lm(y ~ -1 +X)         # We use lm() to estimate a linear regression model, mod, with an intercept 
  res = mod$residuals         # res is assigned the value of the residuals of mod
  res2 = res^2
  
  ln_res2 = lm(log(res2) ~  -1 +X)        # We use the lm() function to estimate a new model
                                          # In this model, the dependent variable is ln(res2), 
                                          # which is explained by X*lambda + v
  lambda_hat = ln_res2$coefficients[2]    # Thereby, the value of lambda_hat is the value of 
                                          # the second estimated coefficient, 
                                          # since the first is the intercept 
  
  # We check if trueVar is TRUE, if so, we assume the structure of the error variance to be exp(x*lambda)
  if(trueVar == TRUE){
    error_cov = matrix(0, N, N)                 # We create an empty square matrix 
    for(i in 1:N){                              # The loop repeats itself for every row of X
      error_cov[i,i] <- exp(X[i,2]*lambda_hat)  # We fill the diagonal in the empty square matrix 
                                                # with the error variance, exp(x*lambda_hat) for each value of x
    }
    
  }
  # We check if trueVar is TRUE, if not, we assume the structure of the error variance to be 1 + x*lambda
  else if(trueVar == FALSE)
  {
    error_cov = matrix(0, N, N)                 # We create an empty square matrix.
    for(i in 1:N){                              # The loop repeats itself for every row of X.
      error_cov[i,i] <- 1 + X[i,2]*lambda_hat   # We fill the diagonal in the empty square matrix 
                                                # with the error variance, 1 + x*lambda_hat, for each value of x
    }
    
  }
beta_fwls = (solve(t(X)%*%solve(error_cov)%*%X)%*%t(X)%*%solve(error_cov)%*%y)[2,1]
# beta_fwls is calculated and returned 

return(beta_fwls)
}
@


<<>>=
#### Data simulation
DataFun <- function(n, lambda) {

    # independent variable
    x <- runif(n, min = 0, max = 2)

    # standard deviation in epsilons normal distribution
    s <- NULL
    for (i in seq_len(n)) {
        s[i] <- exp(x[i]*lambda)
    }

    # error term
    epsilon <- rnorm(n, mean = rep(0, n), sd = s)

    beta <- 2

    # dependent variable
    y <- NULL
    for (i in seq_len(n)) {
        y[i] <- beta*x[i] + epsilon[i]
    }

    # container matrix
    mat <- matrix(data = 0, ncol = 2, nrow = n)

    # creating matrix of indepedent and dependent variables
    for (i in seq_len(n)) {
        mat[i, 1] <- y[i]
        mat[i, 2] <- x[i]
    }
    return(mat)
}
@

Data generation...

The DataFun funktion generates random data with an error term dependent on the covariance strukture \lambda and the number of observations chosen. The funktion starts by generating the standard deviation of epsilon since the error term is dependent on x, which comes from a uniform distrubtion. When all the parameters are calculated the funktion calculates y and then all the different values of y and x are saved in a matrix and that matrix is set to be the return value.


<<>>=
SimFun <- function(n, sim_reps, seed, lambda) {
    set.seed(seed)
    R <- sim_reps

    # saving betas
    mat <- matrix(0, nrow = R, ncol = 4)

    for (i in seq_len(R)) {
    # data sim
    dat <- DataFun(n, lambda)
    # estimate sim
    mat[i,1] <- olsFun(dat)
    mat[i,2] <- wlsFun(dat, lambda)
    mat[i,4]<- fwlsFun(dat, trueVar = TRUE)
    mat[i,3]<- fwlsFun(dat, trueVar = FALSE)
    }
    betas <- apply(mat, 2, var)

    return(betas)
}
@

The funktion SimFun uses all the pre-existing funktions to generate data and fit models depending on the number of simulationsone wants to perform. The funktion then returns all the fitted betas from all of the models. Except from number of simulation one can also freely choose the number of observations for the data aswell, the seed number and the value of \lambda. 

<<>>=
##### 4. Plot variance estimates
x <- c(25, 50, 100, 200, 400)

var_obs <- matrix(0, ncol = 4, nrow = 5)

for (i in seq_along(x)) {
    var_obs[i,] <- (SimFun(x[i], 100, 2020, 2))
}

var_obs = as.data.frame(var_obs)
rownames(var_obs) = c(25, 50, 100, 200, 400)


colnames(var_obs) = c("OLS","WLS","FWLST","FWLSF")
rownames(var_obs) = c("25","50","100","200","400")

ggplot(as.data.frame(var_obs),aes(x=as.numeric(rownames(var_obs)))) +
  geom_line(aes(y = OLS, colour = "OLS")) + 
  geom_line(aes(y = WLS, colour = "WLS")) +
  geom_line(aes(y = FWLST, colour = "FWLST")) + 
  geom_line(aes(y = FWLSF, colour = "FWLSF")) +
  labs(x="Number of obsvervations",y="Variance",
       title="Variance estimates of beta for each function vs sample size") +
  theme(legend.title = element_blank()) +
  theme(panel.grid.minor = element_blank()) +
  scale_x_continuous(breaks=c(25,50,100,200,400),limits=c(25, 400))

#############
@

In figure 1 the variance estimates for each beta coefficient for different sample sizes is presented. We can see that the variance decrease for all functions 
decreases the larger the sample size is. 

=======
\section{EM algorithm for mixture of normals}

<<>>=
##################################################
# start of part2
##################################################
galaxies <- as.data.frame(galaxies)         
names(galaxies) <- "km"

ggplot(galaxies, aes(x = km)) +
  geom_density()

galaxies = galaxies$km

# Function for gamma update 
gammaUpdate = function(x, mu, sigma, pi){
  pdf = t(sapply(x, dnorm, mean = mu, sd = sigma))    
  # pdf is assigned the value of the pdf of the normal distribution, 
  # with mean=mu and standard deviation=sigma evaluated at each x
  # this creates a n*K matrix where n = #observations in data and 
  # K = #components in the mixture

  # we create a for loop that for each row...  
  for(n in 1:length(x)){
    # ... goes through every component 
    for(k in 1:length(mu)){
      # each value of pdf is multiplied by pi  
      pdf[n, k] = pi[k]*pdf[n, k]      
    }
  }
  # an empty matrix created 
  gamma = as.data.frame(matrix(NA, ncol = length(pi), nrow = length(x)))
  
  # we create a loop that for each row...
  for(n in 1:length(x)){
    # ... goes through every component 
    for(k in 1:length(mu)){
      # gamma[n,k] is assigned the value of pdf[n,k] divided by 
      # the sum of all values on the same row
      gamma[n, k] = pdf[n, k]/sum(pdf[n,])
    }
  }
  # This creates a n*K matrix filled with the gamma-values for each x
  return(as.data.frame(gamma))
}

# mu
muUpdate = function(x, gamma){
  K <- ncol(gamma)
  mu <- NULL
  for (i in seq_len(K)) {
    mu[i] <- sum(gamma[,i]*x)/sum(gamma[,i])
  }
  return(mu)
}


### Sigma
sigmaUpdate = function(x, gamma, mu){
  N = matrix(0, ncol= ncol(gamma))
  sigma = matrix(0, ncol = ncol(gamma))
  for(k in 1:ncol(gamma)){
    for(n in 1:length(x)){
      sigma[k] = sigma[k] + gamma[n,k]*(x[n]-mu[k])^2
      N[k] = N[k] + gamma[n,k]
    }
    sigma[k] = sqrt(sigma[k]/N[k])
  }
  return(sigma)
}

piUpdate = function(gamma){
  pi <- NULL
  for (i in 1:ncol(gamma)) {
    pi[i] <- sum(gamma[,i])/sum(gamma)
  }
  return(pi)
}

### A function for log-likelihood 
loglik = function(x, pi, mu, sigma){
  sum_pdf = matrix(0, nrow = length(x))     # A column vector of zeros
  loglike = 0     # To make sure loglike is 0
  
  # A for loop that for each row...
  for(n in 1:length(x)){
    # ... goes through every cluster 
    for(k in 1:length(pi)){
      sum_pdf[n] = sum_pdf[n] + (pi[k] * dnorm(x[n], mu[k], sigma[k]))
      # for this row, n, we add the value of the pdf of the normal distribution, 
      # with mean=mu[k] and standard deviation=sigma[k] evaluated at x[n], 
      # multiplied by pi[k], to sum_pdf[n]
    }
    # we take the log of this sum and add it to loglike 
    loglike = loglike + log(sum_pdf[n])
    # the loop is then started again 
  }
  # When the loop is finished, loglike has the value of the log-likelihood 
  return(loglike)
}

initialValues = function(x, K, reps = 100){
  mu = rnorm(K, mean(x), 5)
  sigma = sqrt(rgamma(K, 5))
  p = runif(K)
  p = p/sum(p)
  currentLogLik = loglik(x, p, mu, sigma)
  for(i in 1:reps){
    mu_temp = rnorm(K, mean(x), 10)
    sigma_temp = sqrt(rgamma(10, 5))
    p_temp = runif(K)
    p_temp = p_temp/sum(p_temp)
    tempLogLik = loglik(x, p_temp, mu_temp, sigma_temp)
    if(tempLogLik > currentLogLik){
      mu = mu_temp
      sigma = sigma_temp
      p = p_temp
      currentLogLik = tempLogLik
    }
  }
  return(list("mu" = mu, "sigma" = sigma, "p" = p))
}

##################################################
# EM algo
##################################################
EM = function(x, K, tol = 0.001){
  inits = initialValues(x, K, 1000)
  mu = inits$mu
  sigma = inits$sigma
  prob = inits$p
  
  prevLoglik <- 0
  loglikDiff<- 1

  # while loop
  while(loglikDiff > tol){
    
    gamma <- gammaUpdate(x, mu, sigma, prob)
    mu <- muUpdate(x, gamma)
    sigma <- sigmaUpdate(x, gamma, mu)
    prob <- piUpdate(gamma)
    
    currentLogLik <- loglik(x, prob, mu, sigma)
    
    loglikDiff <- abs(prevLoglik - currentLogLik)
    
    prevLoglik <- currentLogLik 

  }
  
  return(list('loglik' = currentLogLik, 'mu' = mu, 'sigma' = sigma, 'prob' = prob))
}

set.seed(1996)

final_plot = matrix(0, ncol = 4, nrow= length(galaxies))    # an empty matrix is created 
loglik_values = NULL      # loglik_values is made sure to be empty

# a for loop that goes through the different values of k, from 2 to 5
for(k in 2:5){
  # for each value of k, the EM-functions generates a new z
  z = EM(galaxies, k)
  
  # loglik_values is assigned the value of the log-likelihood of the function
  loglik_values[(k-1)] = z$loglik
  
  # a for loop that repeats k times and sum the value of the pdf of the normal distribution, 
  # with mean=mu[i] and standard deviation=sigma[i] evaluated at x, multiplied by pi[k].
  for(i in 1:k){
    final_plot[,(k-1)] = final_plot[,(k-1)] + z$prob[i] * dnorm(galaxies, z$mu[i], z$sigma[i])
  }
}

loglik_values <- as.data.frame(loglik_values) %>%
  mutate("K" = (2:5))

loglik_plot = ggplot(data = loglik_values) +
  geom_line(aes(x=K, y=loglik_values), color = "blue") +
  labs(x="Number of components",y= "Log likelihood")
loglik_plot

final_plot = as.data.frame(final_plot)
colnames(final_plot) = c("K = 2", "K = 3", "K = 4", "K = 5")
final_plot = cbind(final_plot, galaxies)

ggplot(data = final_plot, aes(x = galaxies)) +
  geom_line(aes(y = `K = 2`, color = "K = 2"), size = 1) + 
  geom_line(aes(y = `K = 3`, color = "K = 3"), size = 1) + 
  geom_line(aes(y = `K = 4`, color = "K = 4"), size = 1) + 
  geom_line(aes(y = `K = 5`, color = "K = 5"), size = 1) + 
  geom_density(aes(fill = "Density plot"), color = "pink", alpha = 0.2, size = 0) + 
  labs(x = "km", y = "Density",title="Plot of different values of K and the density plot of galaxies") +
  theme(legend.title = element_blank(),legend.position = c(.95, .95),
        legend.justification = c("right", "top"),
        legend.box.just = "right",
        legend.margin = margin(6, 6, 6, 6)) 

@

\end{document}
